# 实时

# 1.0 实时数据采集

1. ##### 在Master主节点使用Flume采集实时数据生成器25001端口的socket数据（实时数据生成器脚本为Master节点/data_log目录下的gen_ds_data_to_socket脚本，该脚本为Master节点本地部署且使用socket传输），将数据存入到Kafka的Topic中（Topic名称为ods_mall_log，分区数为4），使用Kafka自带的消费者消费ods_mall_log中的数据，查看Topic 中的前1 条数据的结果，将查看命令与结果完整的截图粘贴至客户端桌面【Release\模块B 提交结果.docx】中对应的任务序号下；

- #### 创建 Flume 配置文件

  ```txt
  a1.channels = c1
  a1.sinks = k1
  a1.sources = s1
  
  a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
  a1.sinks.k1.topic = ods_mall_log
  a1.sinks.k1.brokerList = 192.168.45.10:9092
  
  a1.sources.s1.type = netcat
  a1.sources.s1.port = 25001
  a1.sources.s1.bind = 127.0.0.1
  
  a1.channels.c1.type = memory
  
  a1.sinks.k1.channel = c1
  a1.sources.s1.channels = c1
  ```
- #### 使用Flume命令启动Flume

  `flume-ng agent -f /配置文件 -c /配置文件所在目录 -n name`
- #### 启动实时数据生成器

  1. `cd /data_log`
  2. `./ gen_ds_data_to_socket`
- #### 查看前一条数据

  1. `cd /opt/module/kafka-2.4.1/bin`
  2. `kafka-console-consumer.sh --bootstrap-server 192.168.45.10:9092 --max-messages 1 --from-beginning --topic ods_mall_log`
     - --max-messages 1 消费一条数据
     - --from-beginning 从最早开始
     - --topic 指定主题

2. ##### 在Master节点进入到maxwell-1.29.0的解压后目录下（在/opt/module），配置相关文件并启动，读取MySQL数据的binlog日志（mysql的binlog相关配置已完毕）到Kafka的Topic中（Topic名称为ods_mall_data，分区数为4）。将order_master和order_detail中的数据创建为fact_order_master和fact_order_detail，存储在Kafka的topic为ods_mall_data中，使用Kafka自带的消费者消费ods_mall_data（Topic）中的数据，查看前2条数据的结果；

- #### 进入 maxwell 目录 conf文件
- #### 复制 `config.properties.example` 并重命名为 config.properties

  `cp ./config.properties.example ./config.properties`
- #### 修改maxwell配置文件


  1. 添加行 kafka_topic=topicName
  2. 修改 Mysql login info
     - user=root
     - password=123456
- #### 如果需要 修改etc/my.conf

  添加行 binlog-do-db=库名称

  然后重启mysql

  `systemctl stop mysqld`

  `systemctl start mysqld`
- #### 启动maxwell

  `./maxwell --config ConfigFilePath`
- #### 在MySQL进行相应的操作


  - 登录MySQL `mysql -uroot -p123456`
  - `use use ds_db01`切换到目标库
  - 创建目标表 字段复制源表

    `create table fact_order_master like oredr_master`

    - 插入数据

    `insert into fact_order_master select * from order_master`
  - 一条 `create table fact_order_detail as select * from order_detail`

# 2.0 实时数据清洗

## 1.第一题

1. ##### 使用Flink 消费Kafka 中topic 为ods_mall_data 的数据，根据数据中不同的表将数据分别分发至kafka 的DWD 层的fact_order_master 、 fact_order_detail 的Topic 中（只获取data 的内容，具体的内容格式请自查，其分区数均为2），其他的表则无需处理。使用Kafka 自带的消费者消 费fact_order_master（Topic）的前1 条数据，将结果截图粘贴至客户端桌面【Release\模块C 提交结果.docx】中对应的任务序号下；

- #### 使用Flink 消费Kafka 中topic 为ods_mall_data 的数据
- ##### 在一切开始之前需要先获取Flink的运行环境

  `val env = StreamExecutionEnvironment.getExecutionEnvironment`


  1. ##### 先创建一个Kafka的Source


```scala
val ks = KafkaSource.builder()
         .setBootstrapServers("192.168.45.10:9092")
         .setValueOnlyDeserializer(new SimpleStringSchema())
         .setTopics("ods_mall_data")
         .setGroupId("kafkafafwf")
         .setStartingOffsets(OffsetsInitializer.earliest())
         .build()
```
  2. ##### 使用Source创建DataStream


```scala
env.fromSource(ks,WatermarkStrategy.noWatermarks(),"kafkasi")
```
  3. ##### 截取data内容


     ```scala
     env.fromSource(ks,WatermarkStrategy.noWatermarks(),"kafkasi")
             .map(f =>{
                 val str = f.split("\"data\":")(1)
     			str.substring(0,str.length-1)
             })
     ```
  4. ##### 创建KafkaSink

       1. 目标主题的数据来源于刚刚mx捕捉的mysql日志数据，有两个表
       2. 使用setTopicSelector可以控制数据流转到哪个主题
       3. 在数据中 分割后其id字段是不一样的，所有可以使用id字段作为 流转条件



```scala
val sinkkafa = KafkaSink.builder()
         .setBootstrapServers("192.168.45.10:9092")
         .setRecordSerializer(KafkaRecordSerializationSchema.builder()
         .setValueSerializationSchema(new SimpleStringSchema())
         .setKeySerializationSchema(new SimpleStringSchema())
         .setTopicSelector(new TopicSelector[String] {
             override def apply(t: String): String = {
                 if(t.contains("order_id")) return "fact_order_master"
                 "fact_order_detail "
             }
         }).build())
```
  5. 使用SinkTo写入Kafka

     ```scala
     val stream = env.fromSource(ks, WatermarkStrategy.noWatermarks(), "kafkasi")
             .map(f => {
                 val str = f.split("\"data\":")(1)
                 str.substring(0, str.length - 1)
             })
     stream.sinkTo(sinkkafa.build())
     ```

## 2 第二题

1. ##### 使用Flink 消费Kafka 中topic 为ods_mall_log 的数据，根据数据中不同的表前缀区分，过滤出product_browse 的数据，将数据分别分发至kafka的DWD 层log_product_browse 的Topic 中，其分区数为2，其他的表则无需处理。使用Kafka 自带的消费者消费log_product_browse（Topic）的前1条数据，将结果截图粘贴至客户端桌面【Release\模块C 提交结果.docx】中对应的任务序号下。

2. ### 该主题数据来源于 数据生成器

   - #### 使用Flink 消费Kafka 中topic 为ods_mall_log 的数据

     1. ##### 创建Kafka数据源

        ```scala
        val ks = KafkaSource.builder()
                .setBootstrapServers("192.168.45.10:9092")
                .setTopics("ods_mall_log")
                .setValueOnlyDeserializer(new SimpleStringSchema())
                .setGroupId("kfffff")
                .build();
        ```
   - #### 根据数据中不同的表前缀区分，过滤出product_browse 的数据

     1. 对Kafka数据源进行过滤

        ```scala
        env.fromSource(ks,WatermarkStrategy.noWatermarks(),"awfawf")
                .filter(f => f.contains("product_browse"))
        ```
   - #### 将数据分别分发至kafka的DWD 层log_product_browse 的Topic 中

     1. ##### 其表内数据是csv格式 需要进行转换 并且按照表字段 添加字段
     2. `product_browse:(9789|6749|1|2024112870294178|'20241121175450');`

        - 创建一个 tableInfo类

          ```scala
          case class tableInfo(log_id:String,
                               product_id : String,
                               customer_id : Int,
                               gen_order:Int,
                               order_sn : String,
                               modified_time : String)
          ```
        - 按照要求生成缺失的字段 在map内

          ```scala
          //log_id	long	自增长id	可以使用随机数（0-9）+MMddHHmmssSSS 代替
          val rand = new Random()
          val log_id = rand.nextInt(10) + "+" + new SimpleDateFormat("MMddHHmmssSSS").format(new Date(System.currentTimeMillis()))
          ```
        - 分割数据

          ```scala
          val strs = f.split("product_browse:\\(")(1).replaceAll("[');]", "").split("\\|")
          ```
        - 组成tableInfo

          ```scala
          val obj = tableInfo(log_id, strs(0), strs(1).toInt, strs(2).toInt, strs(3), strs(4))
          ```
        - 将该对象转换为json

          ```scala
          val gson = new Gson()
          gson.toJson(obj)
          ```
        - 创建KafkaSink

          ```scala
          val ksink = KafkaSink.builder()
                  .setBootstrapServers("192.168.45.10:9092")
                  .setRecordSerializer(KafkaRecordSerializationSchema.builder()
                  .setValueSerializationSchema(new SimpleStringSchema())
                  .setKeySerializationSchema(new SimpleStringSchema())
                  .setTopic("log_product_browse").build())
                  .build()
          ```
        - 写入

          ```scala
          .sinkTo(ksink)
          ```
        - map 全部代码

          ```scala
          env.fromSource(ks,WatermarkStrategy.noWatermarks(),"awfawf")
                  .filter(f => f.contains("product_browse"))
                  .map(f =>{
                      val rand = new Random()
                      val log_id = rand.nextInt(10) + "+" + new SimpleDateFormat("MMddHHmmssSSS").format(new Date(System.currentTimeMillis()))
                      val strs = f.split("product_browse:\\(")(1).replaceAll("[');]", "").split("\\|")
                      val obj = tableInfo(log_id, strs(0), strs(1).toInt, strs(2).toInt, strs(3), strs(4))
                      val gson = new Gson()
                      gson.toJson(obj)
                  }).sinkTo(ksink)
          ```
   
   
   
## 3 第三题

3. ##### 在任务1 、2 进行的同时， 需要将order_master 、order_detail 、product_browse 备份至HBase 中（若Int 类型长度不够，可使用BigInt 或Long 类型代替），使用HBase Shell 查看ods:order_master 表的任意2 条数据，查看字段为row_key 与shipping_user、查看ods:order_detail 表的任意2 条数据， 查看字段为row_key 与product_name 、查看ods:product_browse 表的任意2 条数据，查看字段为row_key 与order_sn。 将结果分别截图粘贴至客户端桌面【Release\模块C 提交结果.docx】中对应的任务序号下（截图中不能有乱码）。

   1. 使用FlinkSQLAPI 创建Kafka数据源
   
      ```scala
      envTable.executeSql(
          """
            |create table kafkaSource(
            | order_id bigint,
            | order_sn string,
            | customer_id bigint,
            | shipping_user string,
            | province string,
            | city string,
            | address string,
            | order_source bigint,
            | payment_method bigint,
            | order_money double,
            | district_money double,
            | shipping_money double,
            | payment_money double,
            | shipping_comp_name string,
            | shipping_sn string,
            | create_time string,
            | shipping_time string,
            | pay_time string,
            | receive_time string,
            | order_status string,
            | order_point bigint,
            | invoice_title string,
            | modified_time string
            |) with (
            | 'connector'='kafka',
            | 'topic'='fact_order_master',
            | 'properties.bootstrap.servers'='192.168.45.10:9092',
            | 'properties.group.id'='kkkkkk',
            | 'scan.startup.mode'='earliest-offset',
            | 'format'='json'
            |)
            |
            |""".stripMargin)
      ```
   2. 使用FlinkSQLAPI 创建HBaseSink表
   
      - row_key 代表HBase表的row_key字段
      - info row 代表HBase的列族名为info row<> 尖括号内的字段就是info列族的子列
      - with 后面跟着数据连接方式
      
      ```scala
      envTable.executeSql(
          """
            |create table HbaseSink(
            | row_key string,
            | info row<
            |     order_id bigint,
            |     order_sn string,
            |     customer_id bigint,
            |     shipping_user string,
            |     province string,
            |     city string,
            |     address string,
            |     order_source bigint,
            |     payment_method bigint,
            |     order_money double,
            |     district_money double,
            |     shipping_money double,
            |     payment_money double,
            |     shipping_comp_name string,
            |     shipping_sn string,
            |     create_time string,
            |     shipping_time string,
            |     pay_time string,
            |     receive_time string,
            |     order_status string,
            |     order_point bigint,
            |     invoice_title string,
            |     modified_time string>)
            |     with (
            |     'connector'='hbase-2.2',
            |     'table-name'='ods:order_master',
            |     'zookeeper.quorum'='192.168.45.10:2181')
            |""".stripMargin)
      ```
   3. 将数据源表写入Hbase
   
      - 使用TableAPI对目标Sink进行插入
      - concat是将多个表达式连接在一起 `as "row_key"` 表明这个字段属于Hbase的row_key
      - row() as "info" 表明这些是 列字段
      - `$`符号是取源表中的指定字段
      
      ```scala
      envTable.from("kafkaSource").select(
                  concat(randInteger(10).cast(DataTypes.STRING()),dateFormat(currentTimestamp(),"yyyyMMddHHmmssSSS")) as "row_key",
                  row(
                      $"order_id",
                      $"order_sn",
                      $"customer_id",
                      $"shipping_user",
                      $"province",
                      $"city",
                      $"address",
                      $"order_source",
                      $"payment_method",
                      $"order_money",
                      $"district_money",
                      $"shipping_money",
                      $"payment_money",
                      $"shipping_comp_name",
                      $"shipping_sn",
                      $"create_time",
                      $"shipping_time",
                      $"pay_time",
                      $"receive_time",
                      $"order_status",
                      $"order_point",
                      $"invoice_title",
                      $"modified_time"
                  ) as "info").executeInsert("HbaseSink")
      ```

# 3.0 实时指标计算

## 1.

1. ##### 使用Flink 消费kafka 中log_product_browse 主题的数据，统计商品的UV（浏览用户量）和PV（商品浏览量），将结果写入HBase 中的表ads:online_uv_pv 中。使用Hive cli(没写错)查询ads.pv_uv_result 表按照product_id 和pv 进行降序排序，查询出10 条数据

1. #### 本参考实现中采取的规则是：rowkey = 随机数（0-9）+ yyyyMMddHHmmssSSS。比赛时以具体要求为准。


   - product_id 产品ID
   - customer_id 用户ID
   - UV 不重复用户访问量
   - PV 总含重复用户访问量
2. 该Topic数据来源于 Mx的日志收集 也就是MySQL

   - 使用Flink消费kafka中的数据

     1. 创建Kafka数据源

        ```scala
        val ks = KafkaSource.builder()
                        .setBootstrapServers("192.168.45.10:9092")
                        .setTopics("log_product_browse")
                        .setGroupId("kkkkkawfa")
                        .setStartingOffsets(OffsetsInitializer.earliest())
                        .setValueOnlyDeserializer(new SimpleStringSchema())
                        .build()
        ```
     
     2. 其存储的是json数据 直接转换为Object 然后取指定字段就可以
     
        ```scala
        .map(f=>{
            val json = JsonParser.parseString(f).getAsJsonObject
            val pid = json.get("product_id").getAsString
            val cid = json.get("customer_id").getAsString
            Tuple2(pid,cid)
        })
        ```
     
     3. 使用KeyBy 将各个 商品分开计算 PV UV
     
        ```scala
        .keyBy(_._1)
        .window(TumblingProcessingTimeWindows.of(Time.seconds(60)))
        //TODO 输入 string string
        //TODO 输出 string string string string
        //TODO Key string
        //TODO WindowType TimeWindow
        .process(new ProcessWindowFunction[(String,String),(String,String,String),String,TimeWindow] {
            override def process(key: String, context: Context, elements: Iterable[(String, String)], out: Collector[(String, String, String)]): Unit = {
                val UV = new util.TreeSet[String]()
                var PV = 0
                val inter = elements.iterator
                while(inter.hasNext){
                    val tuple = inter.next()
                    UV.add(tuple._2)
                    PV +=1
                }
                out.collect(key,UV.toString,PV.toString)
            }
        })
        ```
     
     4. 创建HbaseSink表
     
        ```sql
        envtable.executeSql(
            """
              |create HbaseT(
              | row_key string,
              | info as row<
              |     product_id bigint,
              |     product_name string,
              |     uv bigint,
              |     pv bigint>
              |) with (
              | 'connector'='hbase-2.2',
              | 'table-name'='ads:online_uv_pv',
              | 'zookeeper.quorum'='192.168.45.10:2181'
              |)
              |
              |""".stripMargin)
        ```
     
     5. 将上面的Kafka流转成表
     
        ```scala
        val table = envtable.fromDataStream(stream)
        envtable.createTemporaryView("kafka",table);
        ```
     
     6. 获取MySQL数据库内的商品名称 来源表 `ds_db01.product_info`
     
        ```scala
        envtable.executeSql(
            """
              |create table mysql(
              | product_id bigint,
              | product_name string
              |) with (
              | 'connector'='jdbc',
              | 'url'='jdbc:mysql://192.168.45.10:3306/ds_db01?uerSSL=false',
              | 'table-name'='product_info',
              | 'username'='root',
              | 'password'='123456'
              |)
              |""".stripMargin)
        ```
     
     7. 将流表与MySQL进行Join 取出**商品名称** 并插入Hbase
     
        ```scala
        envtable.sqlQuery(
            """
              |with temp as(
              |select ka._1, ka._2, my.product_name, ka._3, ka._4
              |from mysql as my
              |join kafka as ka
              |on my.product_id = ka._2)
              |select _1 as row_key,
              |row(_2, product_name, _3, _4) as info
              |from temp
              |""".stripMargin).executeInsert("HbaseT")
        ```
     
     8. 在Hbase创建指定表
     
        ```sh
        hbase shell
        create 'ads:online_uv_pv','info'
        ```
     
     9. 在HiveCLI创建关联表
     
        ```hive
        CREATE EXTERNAL TABLE ads.online_uv_pv(
            key STRING,
            product_id BIGINT,
            product_name STRING,
            uv BIGINT,
            pv BIGINT
        ) ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe'
        STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
        with serdeproperties('hbase.columns.mapping'=':key,info:product_id,info:product_name,info:uv,info:pv')
        tblproperties('hbase.table.name'='ads:online_uv_pv','hbase.table.default.storage.type'='binary');
        ```
     
     10. 设置`set hive.exec.mode.local.auto=true;` 设置hive运行模式为本地、
     
     11. 查询`select * from ads.online_uv_pv order by product_id desc,pv desc limit 10;`
   
   ​     
